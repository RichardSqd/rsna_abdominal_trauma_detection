{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad11c9-2a6e-4f66-9cc4-27cefe23676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "import nibabel as nib  # to read NIFTI file\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from nibabel.testing import data_path\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "import pydicom as dicom\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "import timm \n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from monai.transforms import Resize\n",
    "import  monai.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b63bc-b417-4895-bba7-014a67ecb9ca",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52743a99-23a6-44a1-8edc-388315b8529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segfile_dir = \"./segmentations\"\n",
    "trainfile_dir = \"./train_images\"\n",
    "\n",
    "seg_out_dir = \"./seg_out/\"\n",
    "clas_in_dir = \"./clas_in/\"\n",
    "model_dir = './models/'\n",
    "log_dir = './log/' \n",
    "\n",
    "image_sizes = [128 , 128, 128] \n",
    "num_slices = 16\n",
    "num_organs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ea6f8-df7c-4f7c-9eba-096743a39fcf",
   "metadata": {},
   "source": [
    "### Loading Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae03986-9a9c-48bd-bf9a-4e2b018fa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv') \n",
    "train_series_meta_df = pd.read_csv('train_series_meta.csv')\n",
    "test_series_meta_df = pd.read_csv('test_series_meta.csv')\n",
    "image_level_labels_df = pd.read_csv('image_level_labels.csv')\n",
    "sample_submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57331501-f861-483f-a069-719279d8d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all train_images and retrive their series_id\n",
    "train_images_files = os.listdir('./train_images')\n",
    "train_images_file_series_id = []\n",
    "p_series_id_dict = {}\n",
    "for pid in train_images_files:\n",
    "    for s in os.listdir('./train_images'+'/'+pid):\n",
    "        train_images_file_series_id.append(int(s))\n",
    "        p_series_id_dict[int(s)] = pid\n",
    "train_images_df = pd.DataFrame({'series_id': train_images_file_series_id})\n",
    "train_images_df['patient_id'] = train_images_df['series_id'].apply(lambda x:p_series_id_dict[x])\n",
    "train_images_df['train_img_file_path'] = train_images_df['series_id'].apply(lambda x: trainfile_dir+\"/\"+str(p_series_id_dict[x])+\"/\"+ str(x)+\"/*\")\n",
    "train_images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca78b2-b687-4312-96b5-a80e864b33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all segmentation files\n",
    "segments = os.listdir('./segmentations')\n",
    "segments_images_df = pd.DataFrame({'seg_files': segments})\n",
    "segments_images_df['series_id'] = segments_images_df['seg_files'].apply(lambda x: int(x[:-4]))\n",
    "segments_images_df['seg_img_file_path'] = segments_images_df['seg_files'].apply(lambda x: segfile_dir+\"/\"+ x)\n",
    "del segments_images_df['seg_files']\n",
    "print(segments_images_df.shape)\n",
    "segments_images_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3471f5-133f-4daf-bc8b-eb7f30568de0",
   "metadata": {},
   "source": [
    "### Load Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ab2b5-2b28-4f26-9ee8-e07d5dce0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNNModel, self).__init__()\n",
    "        #?\n",
    "        self.n_blocks = 4 \n",
    "        #doc: https://timm.fast.ai/create_model\n",
    "        self.encoder = timm.create_model(\n",
    "            \"resnet18d\",\n",
    "            in_chans = 3,\n",
    "            features_only = True, ######\n",
    "            pretrained=False,\n",
    "            drop_rate = 0\n",
    "        )\n",
    "        \n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        #print(g.shape[1])\n",
    "        \n",
    "        '''\n",
    "        torch.Size([1, 64, 32, 32])\n",
    "        torch.Size([1, 256, 16, 16])\n",
    "        torch.Size([1, 512, 8, 8])\n",
    "        torch.Size([1, 1024, 4, 4])\n",
    "        torch.Size([1, 2048, 2, 2])\n",
    "        64 256 512 1024 2048 \n",
    "        '''\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "\n",
    "        #example : https://smp.readthedocs.io/en/v0.1.3/_modules/segmentation_models_pytorch/unet/model.html\n",
    "        #blocks->depth of the unet\n",
    "        self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:self.n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:self.n_blocks],\n",
    "                n_blocks=self.n_blocks,\n",
    "            )\n",
    "        out_dim = 5 #features \n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[self.n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        global_features = [0] + self.encoder(x)[:self.n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15777ae8-4e4d-4ec3-a49c-b7ba787ed236",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df = segments_images_df.merge(train_images_df, on='series_id', how='left')\n",
    "seg_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8bde55-fa05-418b-83c6-229d256d1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation \n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i, (_, v_ind) in enumerate(kf.split(seg_train_df)):\n",
    "    seg_train_df.loc[seg_train_df.index[v_ind], 'kfold'] = i\n",
    "print(seg_train_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ee295-2131-4e34-99c3-2ace442087c6",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812edc97-88ef-4230-9f87-a48e23195183",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform_train  = transforms.Compose([\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
    "    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
    "    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
    "])\n",
    "\n",
    "img_transform_valid = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68a6f2-5370-4e6e-9dd1-6325d3fc6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "\n",
    "def norm_img(imgs):\n",
    "    imgs = imgs - np.min(imgs)\n",
    "    imgs = imgs / (np.max(imgs) + 1e-4)\n",
    "    imgs = (imgs * 255).astype(np.uint8)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def load_dicom(train_img_file_path):\n",
    "    train_images_glob = glob(train_img_file_path)\n",
    "    #sort images into ascending order \n",
    "    train_images_glob = sorted(train_images_glob,key=lambda x: int(x.split('\\\\')[-1].split('.dcm')[0]))  \n",
    "    selected_z_indices = np.quantile(list(range(len(train_images_glob))), np.linspace(0., 1., image_sizes[2])).round().astype(int)\n",
    "    imgs = []\n",
    "\n",
    "    for i in selected_z_indices:\n",
    "        img_path = train_images_glob[i]\n",
    "        img = dicom.dcmread(img_path).pixel_array\n",
    "        img_resized = cv2.resize(img, (image_sizes[0], image_sizes[1]), interpolation = cv2.INTER_CUBIC)  \n",
    "        img_resized = np.uint16(img_resized)\n",
    "        img_enhanced = clahe.apply(img_resized)\n",
    "        imgs.append(img_enhanced)\n",
    "    # DO DICOM IMG PRE-PROCESSING HERE\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    imgs = np.stack(imgs, -1)\n",
    "    #normalize the images\n",
    "    imgs = norm_img(imgs)\n",
    "    imgs = np.expand_dims(imgs, 0).repeat(3, 0) #to 3 channels\n",
    "    return imgs\n",
    "    \n",
    "\n",
    "def load_seg_nii(path):\n",
    "    #https://nipy.org/nibabel/nibabel_images.html\n",
    "    img = nib.load(path).get_fdata()\n",
    "    img = img.transpose(1, 0, 2)[::-1, :, ::-1]  \n",
    "\n",
    "    #print(img[0])\n",
    "    #print(img[:,:,0])\n",
    "    shape = img.shape\n",
    "\n",
    "    \n",
    "    mask = np.zeros((5, shape[0], shape[1], shape[2]))\n",
    "    \n",
    "    for cid in range(1):\n",
    "        mask[cid] = (img == (cid+1))\n",
    "\n",
    "    #img = cv2.resize(img[:,:,1 + 0], (image_sizes[0], image_sizes[1]), interpolation = cv2.INTER_LINEAR)  \n",
    "    #i1 = rotate(img, -90)\n",
    "    #i1 = cv2.flip(i1, 1)\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    \n",
    "    #doc: https://docs.monai.io/en/stable/transforms.html\n",
    "    #print(\"ori size\")\n",
    "    #print(mask.shape)\n",
    "    mask = Resize(spatial_size=(128,128,128))(mask)\n",
    "    #print(mask.shape)\n",
    "    return norm_img(mask)\n",
    "\n",
    "    \n",
    "\n",
    "class SEGDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, transform):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        train_img_file_path = row['train_img_file_path']\n",
    "        seg_img_file_path = row['seg_img_file_path']\n",
    "        img = load_dicom(train_img_file_path)\n",
    "        mask = load_seg_nii(seg_img_file_path)\n",
    "\n",
    "\n",
    "        res = self.transform({'image':img, 'mask':mask})\n",
    "        img = res['image'] / 255.\n",
    "        mask = res['mask']\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        print(img.shape)\n",
    "        img, mask = torch.tensor(img).float(), torch.tensor(mask).float()\n",
    "        \n",
    "        return img, mask \n",
    "\n",
    "\n",
    "class SEGTestDatasetSingle(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe  #.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[2]\n",
    "        #print(row)\n",
    "        series_id = (row['series_id'])\n",
    "        train_img_file_path = (row['train_img_file_path'])\n",
    "        print(train_img_file_path)\n",
    "        print(type(train_img_file_path))\n",
    "        img = load_dicom(train_img_file_path) \n",
    "        img = img.astype(np.float32)  # to 3ch\n",
    "        img = img / 255.\n",
    "        return series_id, train_img_file_path, torch.tensor(img).float()\n",
    "\n",
    "class SEGTestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe  #.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        #print(row)\n",
    "        series_id = (row['series_id'])\n",
    "        train_img_file_path = (row['train_img_file_path'])\n",
    "        #print(train_img_file_path)\n",
    "        #print(type(train_img_file_path))\n",
    "        img = load_dicom(train_img_file_path) \n",
    "        img = img.astype(np.float32)  # to 3ch\n",
    "        img = img / 255.\n",
    "        return series_id, train_img_file_path, torch.tensor(img).float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b0100-3d53-42b4-8f12-453f360767ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b71c55-383d-4b68-8f19-8469ae74a885",
   "metadata": {},
   "source": [
    "### Prediction Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee19ce-2d5a-4b9a-ac8d-4f80b4b329a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "kernel_type = 'timm3d_res50d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "p_mixup = 0.1\n",
    "batch_size = 1\n",
    "num_workers = 0 \n",
    "device = torch.device('cuda')\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539757e-fbdc-46e2-aeb9-027892530374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "load_segmodel_path =  os.path.join(model_dir+ f'{kernel_type}_fold{fold}_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd66739-46b0-4860-91e0-dc45a1139326",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(load_segmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227bfb9-74c4-4fb6-a0c3-5b81dcd5b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model_state_dict' in sd.keys():\n",
    "    sd = sd['model_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d90d88-bc1e-420f-95fb-5a95ee559f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "lm = SegNNModel()\n",
    "lm = convert_3d(lm)\n",
    "lm = lm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ad658-41b0-41f8-8d96-504b485fa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.load_state_dict(sd, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde9336-6c58-4c7f-957b-12bdf26f42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5664502-330e-4fff-9d84-953dd6d927f4",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e33e6-0469-4e88-9f97-4c611c6cd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_dataset = SEGTestDatasetSingle(seg_train_df)\n",
    "local_loader_train = torch.utils.data.DataLoader(seg_train_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c86f4c-3c92-4e1f-9963-227678f6acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7abbd5-c150-4568-a02f-55c0fd5c4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_dicom_imgs(msk, organ_id, t_paths, cropped_images):\n",
    "    n_scans = len(t_paths)\n",
    "    organ = []\n",
    "    try:\n",
    "        msk_b = msk[organ_id] > 0.2\n",
    "        msk_c = msk[organ_id] > 0.05\n",
    "\n",
    "        x = np.where(msk_b.sum(1).sum(1) > 0)[0]\n",
    "        y = np.where(msk_b.sum(0).sum(1) > 0)[0]\n",
    "        z = np.where(msk_b.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0 or len(z) == 0:\n",
    "            x = np.where(msk_c.sum(1).sum(1) > 0)[0]\n",
    "            y = np.where(msk_c.sum(0).sum(1) > 0)[0]\n",
    "            z = np.where(msk_c.sum(0).sum(0) > 0)[0]\n",
    "\n",
    "        x1, x2 = max(0, x[0] - 1), min(msk.shape[1], x[-1] + 1)\n",
    "        y1, y2 = max(0, y[0] - 1), min(msk.shape[2], y[-1] + 1)\n",
    "        z1, z2 = max(0, z[0] - 1), min(msk.shape[3], z[-1] + 1)\n",
    "\n",
    "        zz1, zz2 = int(z1 / msk_size * n_scans), int(z2 / msk_size * n_scans)\n",
    "\n",
    "        inds = np.linspace(zz1 ,zz2-1 ,n_slice).astype(int)\n",
    "        inds_ = np.linspace(z1 ,z2-1 ,n_slice).astype(int)\n",
    "        \n",
    "        for sid, (ind, ind_) in enumerate(zip(inds, inds_)):\n",
    "            msk_this = msk[organ_id, :, :, ind_]\n",
    "\n",
    "            images = []\n",
    "            for i in range(-n_ch//2+1, n_ch//2+1):\n",
    "                try:\n",
    "                    loaded_dicom = dicom.read_file(t_paths[ind+i])\n",
    "                    \n",
    "                    images.append(loaded_dicom.pixel_array)\n",
    "                except Exception as error:\n",
    "                    #print(error)\n",
    "                    #print(\"reaching the end of this dicom file, so null img is used...\")\n",
    "                    images.append(np.zeros((512, 512)))\n",
    "\n",
    "            data = np.stack(images, -1)\n",
    "            data = data - np.min(data)\n",
    "            data = data / (np.max(data) + 1e-4)\n",
    "            data = (data * 255).astype(np.uint8)\n",
    "            #x,y cut \n",
    "            \n",
    "            msk_this = msk_this[x1:x2, y1:y2]\n",
    "            xx1 = int(x1 / msk_size * data.shape[0])\n",
    "            xx2 = int(x2 / msk_size * data.shape[0])\n",
    "            yy1 = int(y1 / msk_size * data.shape[1])\n",
    "            yy2 = int(y2 / msk_size * data.shape[1])\n",
    "            \n",
    "            data = data[xx1:xx2, yy1:yy2]\n",
    "            data = np.stack([cv2.resize(data[:, :, i], (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR) for i in range(n_ch)], -1)\n",
    "            msk_this = (msk_this)#.astype(np.uint8)\n",
    "            msk_this = cv2.resize(msk_this, (image_size_cls, image_size_cls), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "            re_msk = msk_this[:, :, np.newaxis]\n",
    "\n",
    "            data = np.concatenate([data, re_msk], -1)\n",
    "            organ.append(torch.tensor(data))\n",
    "\n",
    "    except Exception as error:\n",
    "        #print(error)\n",
    "        for sid in range(n_slice):\n",
    "            organ.append(torch.ones((image_size_cls, image_size_cls, n_ch+1)).int())\n",
    "\n",
    "    cropped_images[organ_id] = torch.stack(organ, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d6f61-8173-47d7-bdbd-f34188a9b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cropped_to_files(sid, data_5d, chan_index):\n",
    "    \n",
    "    for organ_index in range(5):\n",
    "        \n",
    "        data_4d = data_5d[organ_index]\n",
    "        for slice_index in range(num_slices):\n",
    "            \n",
    "            crop_img_slice = data_4d[slice_index][:,:,chan_index]\n",
    "            \n",
    "            crop_clas_in_dir = clas_in_dir+ '/'+ str(int(sid)) +'/' \n",
    "            crop_clas_in_path = crop_clas_in_dir + \"organ_id_\"+ str(organ_index) +'_'+ \"slice_id_\"+  str(slice_index)+'_'+ \"chan_id_\"+str(chan_index) + '.png'\n",
    "            #image = Image.fromarray(seg_img_slice)\n",
    "            \n",
    "            #plt.imshow(seg_img_slice)  # Use 'gray' colormap for grayscale images\n",
    "\n",
    "            if not os.path.exists(crop_clas_in_dir):\n",
    "                os.makedirs(crop_clas_in_dir)\n",
    "            plt.imsave(crop_clas_in_path, crop_img_slice)\n",
    "\n",
    "def save_cropped_to_files_all_chans(sid, data_5d):\n",
    "    \n",
    "    for organ_index in range(5):\n",
    "        \n",
    "        data_4d = data_5d[organ_index]\n",
    "        for slice_index in range(num_slices):\n",
    "            \n",
    "            crop_img_slices = data_4d[slice_index][:,:,:]\n",
    "            \n",
    "            crop_clas_in_dir = clas_in_dir+ '/'+ str(int(sid)) +'/' \n",
    "            crop_clas_in_path = crop_clas_in_dir + \"organ_id_\"+ str(organ_index) +'_'+ \"slice_id_\"+  str(slice_index)+'_'+ \"chan_id_\"+str(chan_index) + '.npy'\n",
    "            #image = Image.fromarray(seg_img_slice)\n",
    "            \n",
    "            #plt.imshow(seg_img_slice)  # Use 'gray' colormap for grayscale images\n",
    "\n",
    "            if not os.path.exists(crop_clas_in_dir):\n",
    "                os.makedirs(crop_clas_in_dir)\n",
    "                \n",
    "            np.save(crop_clas_in_path, crop_img_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b178a-b8fd-4a7f-9a23-05a978883b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ch = 6\n",
    "n_slice = 16\n",
    "image_size_cls = 224\n",
    "msk_size = 128\n",
    "\n",
    "\n",
    "def process_imgs_to_files(msk, sid, train_img_file_path ):\n",
    "    \n",
    "    train_images_glob = glob(train_img_file_path)\n",
    "    #sort images into ascending order \n",
    "    t_paths =  sorted(train_images_glob,key=lambda x: int(x.split('\\\\')[-1].split('.dcm')[0]))   \n",
    "    cropped_images = [[None] * 5]\n",
    "    \n",
    "    #print(\"cropping begins\")\n",
    "    for organ_id in range(5):\n",
    "        crop_dicom_imgs(msk,organ_id,t_paths,cropped_images[0])\n",
    "        \n",
    "    #print(\"saving begins\")\n",
    "    #for chan_index in range(n_ch+1):\n",
    "        #save_cropped_to_files(sid, cropped_images[0], chan_index)\n",
    "    save_cropped_to_files_all_chans(sid, cropped_images[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1ef4f-9ebc-44bc-804d-4f686a08d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ori_img = None\n",
    "sid = None \n",
    "i = 0 \n",
    "with torch.no_grad():\n",
    "    \n",
    "    for series_id, train_img_file_path, images in local_loader_train:\n",
    "        train_img_file_path = train_img_file_path[0]\n",
    "        ori_img = images\n",
    "        images = images.cuda()\n",
    "        outputs = lm(images)\n",
    "        print(\"output processed\")\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "        #msk = predictions[0].copy()\n",
    "\n",
    "        msk = np.array(outputs.cpu()[0])\n",
    "        process_imgs_to_files(msk,series_id,train_img_file_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52b7ea-dfa8-4c97-979f-e6a1de4e5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid,organ_id,                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf54055-df0e-43fe-80c6-2a0e45e06cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9633b32-ebdf-4f11-9920-4bc58a89961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = ori_img[0]\n",
    "#plt.imshow(a[:,:,50]) \n",
    "#plt.show()\n",
    "plt.imshow(a[0][:,:, 50]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d3292-e949-4802-9d2f-971a5beb96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = ori_img[0]\n",
    "#plt.imshow(a[:,:,50]) \n",
    "#plt.show()\n",
    "plt.imshow(a[0][:,:, 72]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a59c18-7bfb-4eb3-ac84-e260b00ad0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = np.array(predictions[0][0])\n",
    "plt.imshow(d[:,:,44]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b847b64-d8a5-41a4-b95c-150a78ae6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "d = np.array(predictions[0][4])\n",
    "plt.imshow(d[:,:,80]) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dc8ea-6ccd-46d7-a9f8-4adf0ca1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d = np.array(predictions[0]).copy()\n",
    "\n",
    "'''\n",
    "for organ_index in range(5):\n",
    "    data_3d = data_4d[organ_index]\n",
    "    for slice_index in range(num_slices):\n",
    "        #print(slice_index * (128//num_slices))\n",
    "        \n",
    "        #seg_img_slice = data_3d[:,:, slice_index * (128//num_slices)]\n",
    "        seg_img_slice = data_4d[0, :, :, 50]\n",
    "        for i in range(len(seg_img_slice)):\n",
    "            for j in range(len(seg_img_slice[0])):\n",
    "                if seg_img_slice[i][j] <= 0.2:\n",
    "                    seg_img_slice[i][j] = 0 \n",
    "\n",
    "\n",
    "        #plt.imshow(seg_img_slice)\n",
    "        #plt.show()\n",
    "        \n",
    "        seg_pred_out_dir = seg_out_dir+ '/'+ str(int(sid)) +'/' \n",
    "        seg_pred_out_path = seg_pred_out_dir + str(organ_index) +'_'+ str(slice_index) + '.png'\n",
    "        #image = Image.fromarray(seg_img_slice)\n",
    "        \n",
    "        #plt.imshow(seg_img_slice)  # Use 'gray' colormap for grayscale images\n",
    "        #plt.axis('off')  # Turn off axis labels and ticks\n",
    "        #plt.show()\n",
    "        if not os.path.exists(seg_pred_out_dir):\n",
    "            os.makedirs(seg_pred_out_dir)\n",
    "        plt.imsave(seg_pred_out_path, seg_img_slice)\n",
    "\n",
    "#plt.imshow() \n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42409ae2-97a6-4d6f-bc5f-6628180a1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data = ori_img[1][0]  # Replace this with your actual data\n",
    "matrix_3d = data.copy()\n",
    "\n",
    "for dim1 in range(len(matrix_3d)):\n",
    "    for dim2 in range(len(matrix_3d[dim1])):\n",
    "        for dim3 in range(len(matrix_3d[dim1][dim2])):\n",
    "            if matrix_3d[dim1][dim2][dim3] and matrix_3d[dim1][dim2][dim3] < 3:\n",
    "                matrix_3d[dim1][dim2][dim3] = None\n",
    "  \n",
    "\n",
    "# Create a meshgrid for the x, y, and z coordinates\n",
    "x = np.arange(128)\n",
    "y = np.arange(128)\n",
    "z = np.arange(128)\n",
    "X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the 3D surface\n",
    "ax.scatter(X, Y, Z, c=matrix_3d, cmap='viridis')  # You can use scatter for point clouds\n",
    "\n",
    "# Add a color bar which maps values to colors\n",
    "fig.colorbar(ax.scatter(X, Y, Z, c=matrix_3d, cmap='viridis'))\n",
    "\n",
    "# Set labels for the axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714188a-60db-4c43-a142-df6394d9cda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
